\chapter{Background}

\section{Introduction}

This chapter reviews the theoretical and methodological background for studying context-dependent interpretation in NLP. Many standard approaches struggle when meaning depends on discourse rather than surface syntax alone, as in anaphora resolution \cite{mitkov_2014}. We therefore focus on four strands of prior work: formal logic, knowledge graphs, neural networks, and benchmarks such as the Winograd Schema.

Formal logic and tableau methods provide the core mechanism for representing structured semantic relations, especially within neo-Davidsonian event semantics \cite{parsons_1990}. Knowledge graphs and their temporal variants capture relations that evolve over time, which is central to modeling events in language \cite{tkgs}. Neural networks with self-attention have transformed NLP, but they still fall short when tasks require deep, context-aware interpretation \cite{vaswani_2023}.

The Winograd Schema Challenge illustrates the need for richer models of reference \cite{winograd}. Its hand-crafted sentences require background knowledge and pragmatic reasoning to resolve ambiguous pronouns, and they motivate the hybrid approach pursued in this thesis.

\section{Synthesis and Positioning}
Hybrid reasoning systems typically fall into two families. One line integrates symbolic search with lightweight, hand-crafted control (e.g., policy comparators over a frontier); another injects learned signals into otherwise symbolic solvers (e.g., SAT-guided or neural-theorem-proving approaches) \cite{selsam_Bjørner_2019}. Both encounter limits: the former needs careful design to remain efficient; the latter can drift when scores correlate with surface cues rather than the structure of the search state.

This thesis positions itself between these strands. It preserves a symbolic calculus and ontology while exposing guidance through an interface that can host either comparators or learned scorers. Unlike neural theorem proving that often supervises truth judgments over clauses end to end, Design~B reads structured event bundles and scores frontier states, which aligns with the decision points that drive tableau expansion. Agreement with a reference policy is therefore a meaningful target, and the approach retains the interpretability of symbolic methods.

\section{Formal Logic}

\subsection{First-Order Logic}

Formal logic, especially first-order logic, has been central in the representation of structured, rule-based reasoning in natural language processing. First-order logic allows for the exact description of sentence structure and the relations between entities, hence providing a base for semantic analysis and logical inference in natural language contexts. Early works by \cite{frege_1879} and later developments by \cite{tarski_56} established the grounding for FOL, which allows the formal expression of propositions concerning entities and their properties and logical manipulation thereof. Thus, FOL is important in NLP for constructing structures that represent relations among the different parts of sentences, and this becomes very relevant in tasks requiring logical inference. Based on FOL, tableau calculus adds a systematic, rule-based method for checking logical consistency of a model. Tableau calculus was introduced by \cite{smullyan_1995}, which allows one to search systematically through the possible interpretations of the logical statements, thus allowing checks for satisfiability and reasoning. The tableau approach has been particularly useful in capturing possible meanings in ambiguous sentences, providing a flexible but rigorous framework for modeling different possible semantic interpretations.

\subsection{Event Semantics}

Event semantics, particularly neo-Davidsonian event semantics, extends the standard First-Order Logic (FOL) setup by focusing on the representation of events as core semantic units \cite{neo_david,parsons_1990,kratzer_1996,dowty_1991}.

Neo-Davidsonian semantics introduces an event variable by which a sentence can be decomposed into specific event-related roles, such as subjects, objects, and actions, while also accommodating modifiers. Such a decomposition into event roles underpins a more fine-grained representation of meaning, wholly consonant with the structure of natural language itself, in which verbs often encapsulate complex relations between agents, actions, and contexts.

Starting from the neo-Davidsonian framework, event semantics makes verb phrases easier to analyse, especially when a clause involves several arguments or complex actions. Instead of tying an agent directly to an object, neo-Davidsonian semantics inserts an event entity and links participants through separate roles. This structure allows additional semantic roles, such as instruments or locations, to appear as event modifiers, which helps capture how sentence meaning changes as the discourse unfolds. Event semantics has therefore proved useful in NLP tasks such as information extraction and event detection, where relations between participants and actions are central \cite{hacquard_2006}.

\subsection{Extensional v.s. Intensional Semantics}

A fundamental distinction in formal semantics is that between extensional and intensional understandings of meaning. Extensional semantics, in its truth-conditional logic, is limited to real, observable relations, insofar as it indexes directly to objects in a given domain \cite{montague_1973}. While extensional logic suffices for atomic propositions, it becomes inadequate when reasoning about context-dependent linguistic components, such as hypothetical or possible facts, ubiquitous in natural languages.
On the other hand, intensional semantics respond to this gap by accounting for context-sensitive interpretations and meanings that exceed observable truth conditions. First theorized by \cite{carnap_2011} and further developed in the work of Montague, intensional semantics allows the representation of possible worlds or states in such a way as to provide a structure supporting meaning grounded in potential or hypothetical contexts \cite{montague_1973}.

This adaptability is important in tasks such as anaphora resolution, where the interpretation of pronouns often depends on tacit presuppositions about the situation or the mental states of the entities. Intensional semantics is then a general framework for the interpretation of language, including those nuances of meaning that are not expressed but inferred from the context. Knowledge graphs are structured models that represent entities and the interrelations between them in an integral form. As discussed in \cite{tkgs}

\section{Knowledge Graphs}
\subsection{Knowledge Graphs Data Structure}
Knowledge Graphs are a graph based data structure with an emphasis on the relations and dependencies involved in a certain domain \cite{tkgs}. Every node in this structure denotes an entity; edges are used for representing relations, building up a network of information that may enhance various tasks within NLP, such as information retrieval and semantic search.
\subsection{Temporal Knowledge Graphs Extension}
Temporal Knowledge Graphs (TKGs) extend static knowledge graphs with time-aware relations that track how events and roles evolve \cite{tkgs}. This perspective mirrors neo-Davidsonian event semantics, which already treats situations, frames, and roles as structured representations unfolding over time. Although the ontology in this thesis remains static and does not implement temporal predicates, TKGs illustrate how richer, time-indexed constraints could further align knowledge-graph guidance with the inherently dynamic nature of discourse.

\section{Neural Networks}

Neural networks for natural language processing have progressed from simple feedforward models to architectures that capture intricate linguistic structure. Key milestones include recurrent networks for sequential data \cite{elman_90}, long short-term memory networks that mitigate long-range dependency issues \cite{hochSchm_97}, and transformers whose self-attention layers process entire sentences at once \cite{vaswani_2023}. Pretrained transformers such as BERT \cite{bert_2019} achieve strong results on tasks ranging from question answering to text generation.

Even so, contemporary networks still lack human-level understanding on context-sensitive tasks such as anaphora resolution. They learn from large static datasets and correlations, which limits their ability to represent meaning directly or adapt to subtle contextual shifts \cite{nlu_2020}. As a consequence they often mis-handle pronouns that rely on implicit world knowledge, including examples from the Winograd Schema Challenge.

Recent work addresses this gap by combining the strengths of formal logic and neural models so that intensional, context-aware reasoning can inform neural representations. Hybrid systems often incorporate heuristic guidance inspired by tableau reasoning \cite{selsam_Bjørner_2019}, which supplies a structured view of discourse while leaving room for data-driven scoring. The overall aim is to ground neural predictions in logic-based constraints and make interpretation more transparent.

\section{The Winograd Schema Challenge}

The Winograd Schema Challenge \cite{winograd} benchmarks human-like reasoning in NLP. Each schema is a short sentence with an ambiguous pronoun that can only be resolved by applying context, background knowledge, or commonsense reasoning. Unlike broader benchmarks that reward pattern matching, the Winograd tasks focus squarely on interpreting meaning in context, particularly for anaphora resolution. Every item permits two readings, and choosing the intended one demands a careful account of how entities and actions relate.

The Winograd Schema Challenge in anaphora resolution displays special characteristics that go beyond the usual language tasks, since it heavily depends on context-dependent interpretations and common sense. Anaphora resolution often requires much more than superficial syntactic knowledge, since it demands the capability of inferring subtle, context-driven meanings. For example, the pronoun ``it`` in the sentence ``The trophy does not fit in the suitcase because it is too small`` refers to ``the suitcase``, a relation that can only be worked out if one knows the typical size relations between trophies and suitcases. This type of resolution actually falls under intensional semantics, where meaning is recovered from implicit knowledge or common sense rather than explicit linguistic cues \cite{hirst_1981}. This reliance on contextual elements and intrinsic semantics reveals the inadequacy of models relying strictly on syntactic methods or statistics. Syntactic parsing can be at most useful in the identification of possible antecedents, but only a model that can synthesize global information with intensional semantics can output resolutions correctly and consistently, especially in cases that are as intricate as the Winograd Schema Challenge.

Most contemporary approaches to anaphora resolution within natural language processing fall short of the requirements of the Winograd Schema Challenge. Traditional rule-based approaches are very accurate but have low generalizability; they fail to handle huge amounts of real-world knowledge and context-dependent inferences \cite{mitkov_2014}. On the other hand, neural network-based models, including transformers, are heavily reliant on large datasets and statistical correlations, which become insufficient for capturing context-dependent and inherent meanings \cite{nlu_2020}. Such models, however, cannot deal with anaphora where either contextual or real-world knowledge would be involved, because they don't have the ability to comprehend meaning beyond mere data correlations.

Recent studies address this gap by pairing neural networks with structured reasoning components, for instance the hybrid systems described by \cite{bosselut_2019}.
However, these models are still in their early stages and mostly lack the strength needed for more advanced semantic understanding in complex linguistic tasks. The Winograd Schema Challenge remains one of the important benchmarks due to its ability to bring out failure on the part of NLP models in understanding inherent semantics and intensional clues crucial for human-like comprehension of natural language.

\section{Gaps in Research}

Existing approaches across formal logic, knowledge graphs, and neural networks each fall short of human-like semantic understanding. Formal logic and tableau calculus offer rigorous reasoning frameworks \cite{smullyan_1995}, but they struggle with context-driven nuance.

Knowledge graphs and their temporal extensions represent entity relations efficiently, yet they only capture commonsense dynamics when supported by extensive manual modelling \cite{tkgs}. Transformer-based neural networks excel at learning patterns from large corpora \cite{vaswani_2023}, but their reliance on correlation constrains performance on intensional tasks such as the Winograd Schema \cite{nlu_2020}. These limitations point toward models that blend symbolic structure with flexible representations. Recent work explores neural systems equipped with logic-based heuristics or temporal knowledge graphs to improve contextual reasoning \cite{selsam_Bjørner_2019}. TKGs in particular align with event semantics by depicting relations that change over time, a property that helps disambiguate references in discourse. This thesis contributes to that trajectory by analysing hybrids that combine symbolic control with learned guidance.
