@incollection{ana-res,
  author    = {Mitkov, Ruslan},
  isbn      = {9780199573691},
  title     = {Anaphora Resolution},
  booktitle = {The Oxford Handbook of Computational Linguistics},
  publisher = {Oxford University Press},
  year      = {2022},
  month     = {06},
  abstract  = {This chapter provides a theoretical background of anaphora and introduces the varieties of this pervasive linguistic phenomenon. Next, it defines the task of anaphora resolution and introduces it as a three-stage process: identification of anaphors, location of the candidates for antecedents, and the resolution algorithm. After that, the chapter outlines a selection of influential and extensively cited anaphora resolution algorithms and proceeds to discuss issues related to the evaluation of anaphora resolution algorithms. Recent deep learning work on anaphora and coreference resolution is briefly presented as well. Finally, the chapter explains why anaphora resolution is important for various NLP applications.},
  doi       = {10.1093/oxfordhb/9780199573691.013.36},
  url       = {https://doi.org/10.1093/oxfordhb/9780199573691.013.36},
  eprint    = {https://academic.oup.com/book/0/chapter/358152193/chapter-pdf/45719936/oxfordhb-9780199573691-e-36.pdf}
}

@book{Kehler_2002,
  author    = {Kehler, Andrew},
  title     = {Coherence, Reference, and the Theory of Grammar},
  publisher = {CSLI Publications},
  address   = {Stanford, CA},
  year      = {2002},
  url       = {https://web.stanford.edu/group/cslipublications/cslipublications/pdf/1575862166.pdf}
}

@article{Grosz_1995,
  author  = {Grosz, Barbara J. and Joshi, Aravind K. and Weinstein, Scott},
  title   = {Centering: A Framework for Modeling the Local Coherence of Discourse},
  journal = {Computational Linguistics},
  year    = {1995},
  volume  = {21},
  number  = {2},
  pages   = {203--225},
  url     = {https://aclanthology.org/J95-2003/}
}

@article{Pan+24,
  author  = {Panchendrarajan, R. and Cambria, Erik},
  title   = {Synergizing Machine Learning and Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing},
  journal = {Engineering Applications of Artificial Intelligence},
  year    = {2024},
  volume  = {134},
  pages   = {108339},
  doi     = {10.1016/j.engappai.2024.108339}
}

@article{tkgs,
  title   = {A survey on knowledge graphs: Representation, acquisition, and applications},
  volume  = {33},
  doi     = {10.1109/tnnls.2021.3070843},
  number  = {2},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  author  = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
  year    = {2022},
  month   = {2},
  pages   = {494-514}
} 

@inproceedings{winograd,
  author  = {Levesque, Hector},
  year    = {2011},
  month   = {01},
  pages   = {},
  title   = {The Winograd Schema Challenge.},
  journal = {AAAI Spring Symposium - Technical Report}
}

@article{neo_david,
  title   = {The logical form of action sentences},
  doi     = {10.2307/jj.13027259.6},
  journal = {The Logic of Decision and Action},
  author  = {Davidson, Donald},
  year    = {1967},
  month   = {11},
  pages   = {81-120}
} 

@article{mitkov_2014,
  doi     = {10.4324/9781315840086},
  journal = {Anaphora resolution},
  author  = {Mitkov, Ruslan},
  year    = {2014},
  month   = {2}
} 

@book{parsons_1990,
  author    = {Terence Parsons},
  editor    = {},
  publisher = {MIT Press},
  title     = {Events in the Semantics of English: A Study in Subatomic Semantics},
  year      = {1990}
}

@misc{vaswani_2023,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}

@book{carnap_2011,
  place     = {Chicago, IL},
  title     = {Meaning and necessity: A study in semantics and modal logic},
  publisher = {The University of Chicago Press},
  author    = {Carnap, Rudolf},
  year      = {2011}
} 

@book{frege_1879,
  title  = {Begriffsschrift, eine der arithmetischen nachgebildete formelsprache des reinen Denkens},
  author = {Frege, Gottlob},
  year   = {1879}
} 

@phdthesis{hacquard_2006,
  journal = {Aspects of modality},
  author  = {Hacquard, Valentine},
  year    = {2006}
} 

@article{montague_1973,
  title   = {The proper treatment of quantification in ordinary English},
  doi     = {10.1007/978-94-010-2506-5_10},
  journal = {Approaches to Natural Language},
  author  = {Montague, Richard},
  year    = {1973},
  pages   = {221--242}
} 

@book{smullyan_1995,
  place     = {New York},
  title     = {First-order logic},
  publisher = {Dover},
  author    = {Smullyan, Raymond M.},
  year      = {1995}
} 


@book{tarski_56,
  address   = {London},
  author    = {Alfred Tarski},
  publisher = {Oxford University Press},
  title     = {Logic, Semantics and Metamathematics},
  year      = {1956}
}

@article{selsam_Bjørner_2019,
  title   = {Guiding high-performance sat solvers with UNSAT-core predictions},
  doi     = {10.1007/978-3-030-24258-9_24},
  journal = {Lecture Notes in Computer Science},
  author  = {Selsam, Daniel and Bjørner, Nikolaj},
  year    = {2019},
  pages   = {336--353}
} 

@article{hochSchm_97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735}
}

@article{elman_90,
  added-at  = {2022-07-15T10:18:24.000+0200},
  author    = {Elman, J. L.},
  biburl    = {https://www.bibsonomy.org/bibtex/26f18bf6ac96b75c3c0c077acd19a42fe/ankeritter},
  interhash = {8770242c4eff4016d2408ab338039b7c},
  intrahash = {6f18bf6ac96b75c3c0c077acd19a42fe},
  journal   = {Cognitive Science},
  keywords  = {},
  pages     = {179-211},
  timestamp = {2022-07-15T10:18:24.000+0200},
  title     = {Finding Structure In Time},
  volume    = 14,
  year      = 1990
}

@misc{bert_2019,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1810.04805}
}

@misc{few_shot_2020,
  title         = {Language Models are Few-Shot Learners},
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year          = {2020},
  eprint        = {2005.14165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2005.14165}
}

@inproceedings{nlu_2020,
  title     = {Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data},
  author    = {Bender, Emily M.  and
               Koller, Alexander},
  editor    = {Jurafsky, Dan  and
               Chai, Joyce  and
               Schluter, Natalie  and
               Tetreault, Joel},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.463},
  doi       = {10.18653/v1/2020.acl-main.463},
  pages     = {5185--5198},
  abstract  = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as {``}understanding{''} language or capturing {``}meaning{''}. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of {``}Taking Stock of Where We{'}ve Been and Where We{'}re Going{''}, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.}
}

@book{hirst_1981,
  place     = {Berlin},
  title     = {Anaphora in natural language understanding: A survey},
  publisher = {Springer-Verlag},
  author    = {Hirst, Graeme},
  year      = {1981}
} 

@article{bosselut_2019,
  title   = {Comet: Commonsense transformers for automatic knowledge graph construction},
  doi     = {10.18653/v1/p19-1470},
  journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author  = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
  year    = {2019}
} 

@incollection{kratzer_1996,
  author    = {Kratzer, Angelika},
  title     = {Severing the External Argument from its Verb},
  booktitle = {Phrase Structure and the Lexicon},
  editor    = {Rooryck, Johan and Zaring, Laurie},
  publisher = {Kluwer Academic Publishers},
  address   = {Dordrecht},
  year      = {1996},
  pages     = {109--137}
}

@article{dowty_1991,
  author  = {Dowty, David},
  title   = {Thematic Proto-Roles and Argument Selection},
  journal = {Language},
  year    = {1991},
  volume  = {67},
  number  = {3},
  pages   = {547--619}
}
